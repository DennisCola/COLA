import streamlit as st
import pandas as pd
import requests
from io import BytesIO
from docx import Document
import google.generativeai as genai
import json

# --- 1. åŸºç¤è¨­å®šèˆ‡ AI åˆå§‹åŒ– ---
st.set_page_config(page_title="ç·šæ§è¡Œç¨‹è½‰è¡¨å·¥å…·", layout="wide")

if "GEMINI_API_KEY" not in st.secrets:
    st.error("è«‹å…ˆåœ¨ Secrets è¨­å®š GEMINI_API_KEY")
    st.stop()

genai.configure(api_key=st.secrets["GEMINI_API_KEY"])
model = genai.GenerativeModel('gemini-1.5-flash')

# Google Sheet è³‡æ–™åº«é€£çµ
SHEET_URL = "https://docs.google.com/spreadsheets/d/1y53LHsJkDx2xA1MsLzkdd5FYQYWcfQrhs2KeSbsKbZk/export?format=xlsx"
# æ‚¨æŒ‡å®šçš„ 11 å€‹æ¨™æº–æ¬„ä½
COLS = ["æ—¥æœŸ", "æ˜ŸæœŸ", "å¤©æ•¸", "è¡Œç¨‹å¤§é»", "åˆé¤", "é¤æ¨™", "æ™šé¤", "é¤æ¨™", "æœ‰æ–™é–€ç¥¨", "æ—…é¤¨", "æ˜Ÿç­‰"]

# --- 2. è³‡æ–™åº«é€£å‹•æª¢æŸ¥ ---
@st.cache_data(ttl=300)
def load_db():
    try:
        r = requests.get(SHEET_URL)
        with BytesIO(r.content) as f:
            # è®€å–ä¸‰å€‹åˆ†é ä»¥ç¢ºä¿é€£å‹•æ­£å¸¸
            df_f = pd.read_excel(f, sheet_name="Fixed")
            df_s = pd.read_excel(f, sheet_name="Shared")
            df_d = pd.read_excel(f, sheet_name="Daily")
            return df_f, df_s, df_d
    except Exception as e:
        return None, None, None

db_fixed, db_shared, db_daily = load_db()

st.title("ğŸ“„ è¡Œç¨‹è‡ªå‹•è½‰è¡¨ (ç´”æ·¨ç‰ˆ)")

if db_fixed is not None:
    st.success("âœ… æˆæœ¬è³‡æ–™åº«é€£å‹•æˆåŠŸ")
else:
    st.error("âŒ è³‡æ–™åº«é€£å‹•å¤±æ•—ï¼Œè«‹æª¢æŸ¥ Google Sheet ç¶²å€æˆ–æ¬Šé™")

# --- 3. Word è®€å–èˆ‡è™•ç† ---
up = st.file_uploader("ä¸Šå‚³è¡Œç¨‹ Word (.docx)", type=["docx"])

if up:
    # æª”æ¡ˆåˆ‡æ›é‚è¼¯ï¼šè‹¥ä¸Šå‚³æ–°æª”æ¡ˆå‰‡æ¸…é™¤èˆŠå¿«å–
    if 'fn' not in st.session_state or st.session_state.fn != up.name:
        st.session_state.fn = up.name
        if 'df' in st.session_state:
            del st.session_state.df

    if 'df' not in st.session_state:
        try:
            doc = Document(up)
            # åƒ…æå–æ–‡å­—æ®µè½ï¼ˆè‡ªå‹•å¿½ç•¥åœ–ç‰‡ï¼‰
            content_list = [p.text.strip() for p in doc.paragraphs if p.text.strip()]
            # æå– Word è¡¨æ ¼å…§çš„æ–‡å­—
            for table in doc.tables:
                for row in table.rows:
                    for cell in row.cells:
                        if cell.text.strip():
                            content_list.append(cell.text.strip())
            
            full_text = "\n".join(content_list)
            
            st.info("ğŸ”„ AI æ­£åœ¨é–±è®€è¡Œç¨‹ä¸¦è½‰æ›æ ¼å¼...")

            # AI æŒ‡ä»¤ï¼šåš´æ ¼è¦æ±‚æ ¼å¼èˆ‡ç•™ç™½
            prompt = f"""
            ä½ æ˜¯ä¸€ä½å°ˆæ¥­ç·šæ§åŠ©ç†ã€‚è«‹é–±è®€è¡Œç¨‹å…§å®¹ï¼Œè½‰æ›ç‚º JSON åˆ—è¡¨ã€‚
            å¿…é ˆåŒ…å«é€™ 11 å€‹æ¬„ä½ï¼š{', '.join(COLS)}ã€‚
            
            ã€è¦å‰‡ã€‘ï¼š
            1. æ‰¾ä¸åˆ°è³‡è¨Šã€è®€ä¸æ‡‚æˆ–è¡Œç¨‹æœªæåŠçš„æ ¼å­ï¼Œè«‹ç›´æ¥ç•™ç©ºå­—ä¸² ""ã€‚
            2. ä¸è¦å¯«ä»»ä½•è§£é‡‹æ€§æ–‡å­—æˆ– "ç„¡"ã€‚
            3. "å¤©æ•¸" æ¬„ä½è«‹å¡«ç´”æ•¸å­—ã€‚
            
            å…§å®¹ï¼š
            {full_text[:3000]}
            """
            
            res = model.generate_content(prompt)
            clean_js = res.text.replace('```json', '').replace('```', '').strip()
            data = json.loads(clean_js)
            
            # è½‰æ›ç‚º DataFrame ä¸¦ç¢ºä¿ 11 æ¬„å®Œæ•´
            df_res
